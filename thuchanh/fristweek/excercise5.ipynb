{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader as DL\n",
    "from torch.utils.data import TensorDataset as TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "$$ DiscoveringActivationFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "leaky_ReLU = nn.LeakyReLU(negative_slope=0.01) \n",
    "# default negative_slope=0.01 is mean slope of x<0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "n_features = 2\n",
    "n_classes = 8\n",
    "\n",
    "model_leaky = nn.Sequential(\n",
    "    nn.Linear(n_features, 8),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, n_classes)\n",
    ")\n",
    "\n",
    "total  = 0\n",
    "for p in model_leaky.parameters():\n",
    "    total += p.numel() # numel() is number of elements in tensor\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Learning Rate And Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the weights of the model with SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optim.SGD(model_leaky.parameters(), lr=0.01, momentum=0.95)\n",
    "#momentum is a hyperparameter that multiplies the gradient \n",
    "# of the previous step before adding the gradient of the current step\n",
    "\n",
    "#bad values can lead to divergence or slow convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Learning rate**                          | **Momentum**                                              |\n",
    "|--------------------------------------------|-----------------------------------------------------------|\n",
    "| Controls the step size                     | Controls the inertia                                       |\n",
    "| Too small leads to long training times     | Null momentum can lead to the optimizer being stuck in a local minimum |\n",
    "| Too high leads to poor performances        | Non-null momentum can help find the function minimum      |\n",
    "| Typical values between 10^(-2) and 10^(-4) | Typical values between 0.85 and 0.99                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer initilization (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1250, grad_fn=<MinBackward1>) tensor(0.1250, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(64, 128)\n",
    "print(layer.weight.min(), layer.weight.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer initilization (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.8476, 0.1572, 0.0792,  ..., 0.2461, 0.1292, 0.9178],\n",
       "        [0.4511, 0.8923, 0.4014,  ..., 0.0015, 0.9416, 0.0273],\n",
       "        [0.5610, 0.9758, 0.2160,  ..., 0.5174, 0.1567, 0.8896],\n",
       "        ...,\n",
       "        [0.5182, 0.3257, 0.5692,  ..., 0.5583, 0.7704, 0.4541],\n",
       "        [0.0565, 0.9794, 0.9163,  ..., 0.2847, 0.2188, 0.1318],\n",
       "        [0.2694, 0.6144, 0.1585,  ..., 0.1855, 0.6918, 0.1816]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Linear(64, 128)\n",
    "nn.init.uniform_(layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6226e-05, grad_fn=<MinBackward1>) tensor(1.0000, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(layer.weight.min(), layer.weight.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer learning and fine tuning (1)\n",
    "# \n",
    "torch.save(layer, 'model_initilization.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/j6plzpv56qlgr09yjhlbprvm0000gp/T/ipykernel_29452/62730774.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  new_layer = torch.load('model_initilization.pth')\n"
     ]
    }
   ],
   "source": [
    "new_layer = torch.load('model_initilization.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=64, out_features=128, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(new_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning and fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_fine_tuning = nn.Sequential(\n",
    "    nn.Linear(64, 128),\n",
    "    nn.Linear(128, 256))\n",
    "\n",
    "for name, param in model_fine_tuning.named_parameters():\n",
    "    if name == \"0.weight\":\n",
    "        param.requires_grad = False\n",
    "\n",
    "#transfer learning and fine tuning\n",
    "# Fine-turning is a technique \n",
    "# that consists of unfreezing the entire (or part of) pre-trained model \n",
    "# and re-training it with a different dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals_target = pd.array(\n",
    "    ['animal_name', 'hair', 'feathers', 'eggs', \n",
    "     'milk', 'predator', 'fins', 'legs', 'tail','type'])\n",
    "\n",
    "animals_data = pd.DataFrame(\n",
    "    [[\"skimmer\",     0,1,1,0,1,0,2,1,2],\n",
    "    [\"gull\",        0,1,1,0,1,0,2,1,2],\n",
    "    [\"seahourse\",   0,0,1,0,0,1,0,1,4],\n",
    "    [\"tuatara\",     0,0,1,0,1,0,4,1,3],\n",
    "    [\"squirrel\",    1,0,0,1,0,0,2,1,1]]\n",
    ")\n",
    "\n",
    "#type key : \n",
    "# 1 = mammal, 2 = bird, 3 = reptile, 4 = fish, \n",
    "# 5 = amphibian, 6 = bug, 7 = invertebrate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 2 1]\n",
      " [0 1 1 0 1 0 2 1]\n",
      " [0 0 1 0 0 1 0 1]\n",
      " [0 0 1 0 1 0 4 1]\n",
      " [1 0 0 1 0 0 2 1]]\n"
     ]
    }
   ],
   "source": [
    "features = animals_data.iloc[:, 1:-1]\n",
    "#this code will select all rows and all columns except the last one\n",
    "\n",
    "X = features.to_numpy()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 4 3 1]\n"
     ]
    }
   ],
   "source": [
    "target = animals_data.iloc[:, -1]\n",
    "y = target.to_numpy()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalling TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalling tensor dataset is a class that wraps a tensor \n",
    "# and allows us to access rows and columns\n",
    "# that we can use to train our model\n",
    "#\n",
    "\n",
    "\n",
    "dataset = TD(torch.tensor(X.astype(np.float32)), torch.tensor(y).float().unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 1., 0., 1., 0., 2., 1.])\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "input_sample,  label_sample = sample\n",
    "\n",
    "print(input_sample)\n",
    "print(label_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalling DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x31b49be00>\n"
     ]
    }
   ],
   "source": [
    "#recall that DataLoader is a class that wraps a dataset and provides\n",
    "# an iterable over the dataset\n",
    "\n",
    "#batch size is the number of samples that will be passed through the model\n",
    "\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "\n",
    "#create a DataLoader \n",
    "dl = DL(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "print(dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 1., 0., 0., 2., 1.],\n",
      "        [0., 0., 1., 0., 1., 0., 4., 1.]])\n",
      "tensor([1., 3.])\n",
      "\n",
      "\n",
      "tensor([[0., 1., 1., 0., 1., 0., 2., 1.],\n",
      "        [0., 1., 1., 0., 1., 0., 2., 1.]])\n",
      "tensor([2., 2.])\n",
      "\n",
      "\n",
      "tensor([[0., 0., 1., 0., 0., 1., 0., 1.]])\n",
      "tensor([4.])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_inpus, batch_labels in dl:\n",
    "    print(batch_inpus)\n",
    "    print(batch_labels)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Eluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x31b1e3ec0>\n"
     ]
    }
   ],
   "source": [
    "print(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conculating training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5.7033\n",
      "Epoch 2/5, Loss: 2.1391\n",
      "Epoch 3/5, Loss: 4.2162\n",
      "Epoch 4/5, Loss: 1.4497\n",
      "Epoch 5/5, Loss: 0.9146\n"
     ]
    }
   ],
   "source": [
    "#raw dateset is usually in three subsets: training, validation, and test\n",
    "#training dataset (80-90% of dataset) is  used to adjust the model's paramenters\n",
    "#validation dataset (10-20% of dataset) is used to fine-tune the model's hyperparameters\n",
    "#testing dataset (5-10% of dataset) is only used once to evaluate the model's performance, final metrics\n",
    "\n",
    "# Define the model, criterion, and optimizer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    training_loss = 0.0\n",
    "    for i, data in enumerate(dl, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        training_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = training_loss / len(dl)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conculating validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Validation Loss: 0.7314\n",
      "Epoch 5/5, Validation Loss: 1.4628\n",
      "Epoch 5/5, Validation Loss: 2.1942\n",
      "Epoch 5/5, Validation Loss: 2.9256\n",
      "Epoch 5/5, Validation Loss: 3.6570\n"
     ]
    }
   ],
   "source": [
    "validation_loss = .0\n",
    "\n",
    "model.eval() # Put model is evaluation mode\n",
    "\n",
    "validationloader = DL(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "for i in range(epochs):\n",
    "    # training_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validationloader, 0):\n",
    "            #run the forward pass\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    epoch_loss = validation_loss / len(validationloader)\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ta có thể so sánh hai phương thức tìm hàm loss trên là so sánh giữa training và validation\n",
    " \n",
    "loss để xem xem model có bị overfitting hay không.\n",
    "\n",
    "  Nếu training loss nhỏ hơn validation loss\n",
    "\n",
    " thì model đang bị overfitting, ngược lại thì model đang bị underfitting.\n",
    "\n",
    "ở đây ta thấy rằng __validation loss__ lớn hơn __training loss__ nên model đang bị \n",
    " _underfitting_\n",
    "\n",
    "![Alt text](/Users/hongviet/Documents/GitHub/Data-Analysis/thuchanh/fristweek/hinh1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
