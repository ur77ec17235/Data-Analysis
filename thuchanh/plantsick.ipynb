{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eceaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import MobileNetV2 # Using MobileNetV2 for efficiency\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Configuration and Constants ---\n",
    "DATA_DIR = './PlantVillage/PlantVillage' # <--- MAKE SURE THIS PATH IS CORRECT\n",
    "IMG_HEIGHT = 128          # Reduced size for faster training, can increase (e.g., 224)\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25               # Number of epochs for initial training\n",
    "FINE_TUNE_EPOCHS = 10     # Number of epochs for fine-tuning\n",
    "LEARNING_RATE = 0.001\n",
    "FINE_TUNE_LEARNING_RATE = 1e-5 # Very low learning rate for fine-tuning\n",
    "VALIDATION_SPLIT = 0.2    # 20% of data for validation\n",
    "SEED = 123                # For reproducibility\n",
    "\n",
    "# Check if data directory exists\n",
    "data_dir_path = pathlib.Path(DATA_DIR)\n",
    "if not data_dir_path.exists():\n",
    "    print(f\"Error: Data directory '{DATA_DIR}' not found.\")\n",
    "    print(\"Please ensure the 'PlantVillage' folder is in the correct location.\")\n",
    "    exit() # Stop execution if data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03048ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from: ./PlantVillage/PlantVillage\n",
      "Image size: 128x128\n",
      "Batch size: 32\n",
      "Found 20638 files belonging to 15 classes.\n",
      "Using 16511 files for training.\n",
      "Found 20638 files belonging to 15 classes.\n",
      "Using 4127 files for validation.\n",
      "\n",
      "Found 15 classes:\n",
      "['Pepper__bell___Bacterial_spot', 'Pepper__bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot'] ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "print(f\"\\nLoading data from: {DATA_DIR}\")\n",
    "print(f\"Image size: {IMG_HEIGHT}x{IMG_WIDTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Use image_dataset_from_directory for efficient loading\n",
    "train_ds = image_dataset_from_directory(\n",
    "    data_dir_path,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='int' # Use integer labels for sparse_categorical_crossentropy\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    data_dir_path,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(f\"\\nFound {num_classes} classes:\")\n",
    "# Print first 10 classes for brevity if too many\n",
    "if num_classes > 10:\n",
    "    print(class_names[:10], \"...\")\n",
    "else:\n",
    "    print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model using MobileNetV2...\n",
      "\n",
      "Initial Model Summary (Base Model Frozen):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_128            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,215</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_128            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │        \u001b[38;5;34m19,215\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,277,199</span> (8.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,277,199\u001b[0m (8.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,215</span> (75.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,215\u001b[0m (75.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Initial Training (Epochs: 25) ---\n",
      "Epoch 1/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 154ms/step - accuracy: 0.5635 - loss: 1.3901 - val_accuracy: 0.8289 - val_loss: 0.5390 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 115ms/step - accuracy: 0.8220 - loss: 0.5488 - val_accuracy: 0.8583 - val_loss: 0.4394 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 117ms/step - accuracy: 0.8435 - loss: 0.4696 - val_accuracy: 0.8672 - val_loss: 0.4177 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 115ms/step - accuracy: 0.8563 - loss: 0.4203 - val_accuracy: 0.8718 - val_loss: 0.3971 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 117ms/step - accuracy: 0.8638 - loss: 0.4120 - val_accuracy: 0.8735 - val_loss: 0.3787 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 129ms/step - accuracy: 0.8668 - loss: 0.3921 - val_accuracy: 0.8662 - val_loss: 0.3795 - learning_rate: 0.0010\n",
      "Epoch 7/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 116ms/step - accuracy: 0.8656 - loss: 0.3858 - val_accuracy: 0.8813 - val_loss: 0.3630 - learning_rate: 0.0010\n",
      "Epoch 8/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 86ms/step - accuracy: 0.8762 - loss: 0.3695 - val_accuracy: 0.8791 - val_loss: 0.3559 - learning_rate: 0.0010\n",
      "Epoch 9/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 74ms/step - accuracy: 0.8765 - loss: 0.3730 - val_accuracy: 0.8803 - val_loss: 0.3532 - learning_rate: 0.0010\n",
      "Epoch 10/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 78ms/step - accuracy: 0.8793 - loss: 0.3553 - val_accuracy: 0.8808 - val_loss: 0.3607 - learning_rate: 0.0010\n",
      "Epoch 11/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 76ms/step - accuracy: 0.8771 - loss: 0.3503 - val_accuracy: 0.8873 - val_loss: 0.3450 - learning_rate: 0.0010\n",
      "Epoch 12/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 76ms/step - accuracy: 0.8775 - loss: 0.3618 - val_accuracy: 0.8924 - val_loss: 0.3313 - learning_rate: 0.0010\n",
      "Epoch 13/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 74ms/step - accuracy: 0.8798 - loss: 0.3468 - val_accuracy: 0.8822 - val_loss: 0.3418 - learning_rate: 0.0010\n",
      "Epoch 14/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 75ms/step - accuracy: 0.8791 - loss: 0.3471 - val_accuracy: 0.8890 - val_loss: 0.3287 - learning_rate: 0.0010\n",
      "Epoch 15/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1353s\u001b[0m 3s/step - accuracy: 0.8796 - loss: 0.3614 - val_accuracy: 0.8844 - val_loss: 0.3441 - learning_rate: 0.0010\n",
      "Epoch 16/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2244s\u001b[0m 4s/step - accuracy: 0.8757 - loss: 0.3633 - val_accuracy: 0.8839 - val_loss: 0.3509 - learning_rate: 0.0010\n",
      "Epoch 17/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8815 - loss: 0.3422\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2725s\u001b[0m 5s/step - accuracy: 0.8815 - loss: 0.3422 - val_accuracy: 0.8883 - val_loss: 0.3359 - learning_rate: 0.0010\n",
      "Epoch 18/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m889s\u001b[0m 78ms/step - accuracy: 0.8890 - loss: 0.3233 - val_accuracy: 0.8859 - val_loss: 0.3420 - learning_rate: 2.0000e-04\n",
      "Epoch 19/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 74ms/step - accuracy: 0.8850 - loss: 0.3346 - val_accuracy: 0.8931 - val_loss: 0.3254 - learning_rate: 2.0000e-04\n",
      "Epoch 20/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 74ms/step - accuracy: 0.8920 - loss: 0.3137 - val_accuracy: 0.8968 - val_loss: 0.3157 - learning_rate: 2.0000e-04\n",
      "Epoch 21/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 72ms/step - accuracy: 0.8858 - loss: 0.3283 - val_accuracy: 0.8917 - val_loss: 0.3206 - learning_rate: 2.0000e-04\n",
      "Epoch 22/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 81ms/step - accuracy: 0.8929 - loss: 0.3093 - val_accuracy: 0.8910 - val_loss: 0.3267 - learning_rate: 2.0000e-04\n",
      "Epoch 23/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8945 - loss: 0.3133\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 78ms/step - accuracy: 0.8945 - loss: 0.3133 - val_accuracy: 0.8914 - val_loss: 0.3232 - learning_rate: 2.0000e-04\n",
      "Epoch 24/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 77ms/step - accuracy: 0.8957 - loss: 0.3056 - val_accuracy: 0.8939 - val_loss: 0.3177 - learning_rate: 4.0000e-05\n",
      "Epoch 25/25\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 77ms/step - accuracy: 0.8958 - loss: 0.3060 - val_accuracy: 0.8946 - val_loss: 0.3147 - learning_rate: 4.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "--- Initial Training Finished in 8216.55 seconds ---\n",
      "\n",
      "--- Starting Fine-Tuning ---\n",
      "Unfreezing layers from index 100 onwards in the base model.\n",
      "\n",
      "Model Summary (During Fine-Tuning):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_128            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,215</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_128            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │        \u001b[38;5;34m19,215\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,277,199</span> (8.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,277,199\u001b[0m (8.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,880,655</span> (7.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,880,655\u001b[0m (7.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">396,544</span> (1.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m396,544\u001b[0m (1.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Continuing Training for Fine-Tuning (Max Epochs: 10) ---\n",
      "Epoch 26/35\n",
      "\u001b[1m451/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m6s\u001b[0m 104ms/step - accuracy: 0.5664 - loss: 2.9109"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Data Augmentation and Preprocessing Layers ---\n",
    "# Create data augmentation layers\n",
    "data_augmentation = Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\", input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "        # layers.RandomContrast(0.1) # Optional: Add more augmentation\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "\n",
    "# Preprocessing layer (part of MobileNetV2 preprocessing)\n",
    "# MobileNetV2 expects inputs in the range [-1, 1]\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "# Configure datasets for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# --- 4. Build Model using Transfer Learning (MobileNetV2) ---\n",
    "print(\"\\nBuilding model using MobileNetV2...\")\n",
    "\n",
    "# Load the base model (pre-trained on ImageNet)\n",
    "# include_top=False removes the final classification layer\n",
    "base_model = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "                         include_top=False,\n",
    "                         weights='imagenet')\n",
    "\n",
    "# Freeze the base model layers initially\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create the full model\n",
    "inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = data_augmentation(inputs)       # Apply augmentation\n",
    "x = preprocess_input(x)             # Apply MobileNetV2 preprocessing\n",
    "x = base_model(x, training=False)   # Run base model (batch norm layers in inference mode)\n",
    "x = layers.GlobalAveragePooling2D()(x) # Pool features\n",
    "x = layers.Dropout(0.3)(x)          # Regularization dropout\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x) # Output layer\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# --- 5. Compile the Model ---\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='sparse_categorical_crossentropy', # Use sparse for integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nInitial Model Summary (Base Model Frozen):\")\n",
    "model.summary()\n",
    "\n",
    "# --- 6. Define Callbacks ---\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "# --- 7. Initial Training (Feature Extraction) ---\n",
    "print(f\"\\n--- Starting Initial Training (Epochs: {EPOCHS}) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "initial_training_time = time.time() - start_time\n",
    "print(f\"--- Initial Training Finished in {initial_training_time:.2f} seconds ---\")\n",
    "\n",
    "# --- 8. Fine-Tuning (Optional but Recommended) ---\n",
    "print(\"\\n--- Starting Fine-Tuning ---\")\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# How many layers to unfreeze? Fine-tune from this layer onwards.\n",
    "# A common practice is to unfreeze the top blocks. For MobileNetV2, ~ last 50 layers.\n",
    "fine_tune_at = 100 # Unfreeze layers from index 100 onwards\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "print(f\"Unfreezing layers from index {fine_tune_at} onwards in the base model.\")\n",
    "\n",
    "# Re-compile the model with a very low learning rate for fine-tuning\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nModel Summary (During Fine-Tuning):\")\n",
    "model.summary() # Note the increased number of trainable parameters\n",
    "\n",
    "# Adjust callbacks for fine-tuning if needed (e.g., lower patience)\n",
    "fine_tune_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True), # Keep similar patience or slightly lower\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1) # More sensitive LR reduction\n",
    "]\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "total_epochs_run_initial = len(history.epoch)\n",
    "print(f\"\\n--- Continuing Training for Fine-Tuning (Max Epochs: {FINE_TUNE_EPOCHS}) ---\")\n",
    "start_time_ft = time.time()\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    epochs=total_epochs_run_initial + FINE_TUNE_EPOCHS, # Total epochs to run up to\n",
    "    initial_epoch=total_epochs_run_initial, # Start counting from where initial training stopped\n",
    "    validation_data=val_ds,\n",
    "    callbacks=fine_tune_callbacks\n",
    ")\n",
    "\n",
    "fine_tuning_time = time.time() - start_time_ft\n",
    "print(f\"--- Fine-Tuning Finished in {fine_tuning_time:.2f} seconds ---\")\n",
    "print(f\"--- Total Training Time: {initial_training_time + fine_tuning_time:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "# --- 9. Evaluate the Model ---\n",
    "print(\"\\n--- Evaluating Final Model ---\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "final_loss, final_accuracy = model.evaluate(val_ds)\n",
    "print(f\"\\nFinal Validation Loss: {final_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "# --- 10. Visualize Results ---\n",
    "\n",
    "# Combine histories for plotting\n",
    "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
    "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
    "loss = history.history['loss'] + history_fine.history['loss']\n",
    "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "# Mark where fine-tuning started\n",
    "plt.axvline(total_epochs_run_initial -1 , linestyle='--', color='r', label='Start Fine-Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "# Mark where fine-tuning started\n",
    "plt.axvline(total_epochs_run_initial -1, linestyle='--', color='r', label='Start Fine-Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.suptitle('Model Training History (Including Fine-Tuning)')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Confusion Matrix and Classification Report ---\n",
    "print(\"\\nGenerating Confusion Matrix and Classification Report...\")\n",
    "\n",
    "# Get predictions for the entire validation set\n",
    "y_pred_probabilities = model.predict(val_ds)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "y_true = []\n",
    "# Iterate over the validation dataset batches\n",
    "for images, labels in val_ds:\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "# Ensure we have the same number of predictions and true labels\n",
    "if len(y_pred) != len(y_true):\n",
    "     print(f\"Warning: Mismatch in prediction ({len(y_pred)}) and true label ({len(y_true)}) counts. Re-extracting true labels.\")\n",
    "     # Re-extract true labels carefully, ensuring order matches prediction if possible\n",
    "     # This might happen if the dataset size isn't perfectly divisible by batch size and predict doesn't drop remainder\n",
    "     y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "     # If still mismatch, there might be an issue with predict() or dataset iteration\n",
    "     if len(y_pred) != len(y_true):\n",
    "         print(\"Error: Cannot align predictions and labels. Skipping Confusion Matrix/Report.\")\n",
    "     else:\n",
    "        print(\"Label count corrected.\")\n",
    "\n",
    "\n",
    "if len(y_pred) == len(y_true):\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(max(12, num_classes // 2), max(10, num_classes // 2.5))) # Adjust size dynamically\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.xticks(rotation=90, fontsize=8) # Rotate labels for readability\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Use zero_division=0 to handle cases where a class might have no predicted samples\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
    "else:\n",
    "    print(\"Skipping Confusion Matrix and Classification Report due to label/prediction count mismatch.\")\n",
    "\n",
    "\n",
    "# --- 11. Save the Final Model (Optional) ---\n",
    "# model.save('plant_village_mobilenetv2_final.h5')\n",
    "# print(\"\\nFinal model saved as 'plant_village_mobilenetv2_final.h5'\")\n",
    "\n",
    "# Or save in SavedModel format (recommended)\n",
    "model.save('plant_village_mobilenetv2_savedmodel')\n",
    "print(\"\\nFinal model saved in SavedModel format as 'plant_village_mobilenetv2_savedmodel'\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Script Execution Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f7134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
